{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import partition\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('data/train.csv')\n",
    "test=pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/khaiii_train.pkl\",\"rb\") as fr:\n",
    "    khaiii_train=pickle.load(fr)\n",
    "with open(\"data/khaiii_test.pkl\",\"rb\") as fr:\n",
    "    khaiii_test=pickle.load(fr)\n",
    "with open(\"data/mecab_train.pkl\",\"rb\") as fr:\n",
    "    mecab_train=pickle.load(fr)\n",
    "with open(\"data/mecab_test.pkl\",\"rb\") as fr:\n",
    "    mecab_test=pickle.load(fr)\n",
    "with open(\"data/okt_train.pkl\",\"rb\") as fr:\n",
    "    okt_train=pickle.load(fr)\n",
    "with open(\"data/okt_test.pkl\",\"rb\") as fr:\n",
    "    okt_test=pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['khaiii']=khaiii_train\n",
    "train['mecab']=mecab_train\n",
    "train['okt']=okt_train\n",
    "\n",
    "test['khaiii']=khaiii_test\n",
    "test['mecab']=mecab_test\n",
    "test['okt']=okt_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['아','휴','아이구','아이쿠','아이고','어','나','우리','저희','따라','의해','을','를','에','의','가','으로','로','에게','뿐이다','의거하여','근거하여','입각하여','기준으로',\n",
    "'예하면','예를 들면','예를 들자면','저','소인','소생','저희','지말고','하지마','하지마라','다른','물론','또한','그리고','비길수 없다','해서는 안된다','뿐만 아니라','만이 아니다',\n",
    "'만은 아니다','막론하고','관계없이','그치지 않다','그러나','그런데','하지만','든간에','논하지 않다','따지지 않다','설사','비록','더라도','아니면','만' '못하다','하는 편이 낫다',\n",
    "'불문하고','향하여','향해서','향하다','쪽으로','틈타','이용하여','타다','오르다','제외하고','이 외에','이 밖에','하여야','비로소','한다면' '몰라도','외에도','이곳','여기','부터',\n",
    "'기점으로,따라서','할 생각이다','하려고하다','이리하여','그리하여','그렇게' '함으로써','하지만','일때','할때','앞에서','중에서','보는데서','으로써','로써','까지','해야한다',\n",
    "'일것이다','반드시','할줄알다','할수있다','할수있어','임에 틀림없다','한다면','등','등등','제','겨우','단지','다만','할뿐','딩동','댕그','대해서','대하여','대하면','훨씬','얼마나',\n",
    "'얼마만큼','얼마큼','남짓','여','얼마간','약간','다소','좀','조금','다수','몇','얼마','지만','하물며','또한','그러나','그렇지만','하지만','이외에도','대해 말하자면','뿐이다','다음에',\n",
    "'반대로','반대로 말하자면','이와 반대로','바꾸어서 말하면','바꾸어서 한다면','만약','그렇지않으면','까악','툭','딱','삐걱거리다','보드득','비걱거리다','꽈당','응당','해야한다',\n",
    "'에 가서','각','각각','여러분','각종','각자','제각기','하도록하다','와','과','그러므로','그래서','고로','한 까닭에','하기 때문에','거니와','이지만','대하여','관하여','관한','과연',\n",
    "'실로','아니나다를가','생각한대로','진짜로','한적이있다','하곤하였다','하','하하','허허','아하','거바','와','오','왜','어째서','무엇때문에','어찌','하겠는가','무슨','어디','어느곳',\n",
    "'더군다나','하물며','더욱이는','어느때','언제','야','이봐','어이','여보시오','흐흐','흥','휴','헉헉','헐떡헐떡','영차','여차','어기여차','끙끙','아야','앗','아야','콸콸','졸졸','좍좍','뚝뚝',\n",
    "'주룩주룩','솨','우르르','그래도','또','그리고','바꾸어말하면','바꾸어말하자면','혹은','혹시','답다','및','그에 따르는','때가 되어','즉','지든지','설령','가령','하더라도','할지라도',\n",
    "'일지라도','지든지','몇','거의,하마터면','인젠','이젠','된바에야','된이상','만큼','어찌됏든','그위에','게다가','점에서 보아','비추어 보아','고려하면','하게될것이다','일것이다',\n",
    "'비교적','좀','보다더','비하면','시키다','하게하다','할만하다','의해서','연이서','이어서','잇따라','뒤따라','뒤이어','결국','의지하여','기대여','통하여','자마자','더욱더','불구하고',\n",
    "'얼마든지','마음대로','주저하지 않고','곧','즉시','바로','당장','하자마자','밖에' '안된다','하면된다','그래','그렇지','요컨대','다시 말하자면','바꿔 말하면','즉','구체적으로',\n",
    "'말하자면','시작하여','시초에','이상','허','헉','허걱','바와같이','해도좋다','해도된다','게다가','더구나','하물며','와르르','팍','퍽','펄렁','동안','이래','하고있었다','이었다','에서',\n",
    "'로부터','까지','예하면','했어요','해요','함께','같이','더불어','마저','마저도','양자','모두','습니다','가까스로','하려고하다','즈음하여','다른','다른 방면으로','해봐요','습니까',\n",
    "'했어요','말할것도 없고','무릎쓰고','개의치않고','하는것만 못하다','하는것이 낫다','매','매번','들','모','어느것','어느','로써','갖고말하자면','어디','어느쪽','어느것','어느해',\n",
    "'어느 년도','라' '해도','언젠가','어떤것','어느것','저기','저쪽','저것','그때','그럼','그러면','요만한걸','그래','그때','저것만큼','그저','이르기까지','할 줄 안다','할 힘이' '있다',\n",
    "'너','너희','당신','어찌','설마','차라리','할지언정','할지라도','할망정','할지언정','구토하다','게우다','토하다','메쓰겁다','옆사람','퉤','쳇','의거하여','근거하여','의해','따라',\n",
    "'힘입어','그','다음','버금','두번째로','기타','첫번째로','나머지는','그중에서','견지에서','형식으로 쓰여','입장에서','위해서','단지','의해되다','하도록시키다','뿐만아니라',\n",
    "'반대로','전후','전자','앞의것','잠시','잠깐','하면서','그렇지만','다음에','그러한즉','그런즉','남들','아무거나','어찌하든지','같다','비슷하다','예컨대','이럴정도로','어떻게',\n",
    "'만약','만일','위에서' '서술한바와같이','인 듯하다','하지 않는다면','만약에','무엇','무슨','어느','어떤','아래윗','조차','한데','그럼에도 불구하고','여전히','심지어','까지도',\n",
    "'조차도','하지 않도록','않기 위하여','때','시각','무렵','시간','동안','어때','어떠한','하여금','네','예','우선','누구','누가' '알겠는가','아무도','줄은모른다','줄은 몰랏다','하는 김에',\n",
    "'겸사겸사','하는바','그런 까닭에','한 이유는','그러니','그러니까','때문에','그','너희','그들','너희들','타인','것','것들','너','위하여','공동으로','동시에','하기 위하여','어찌하여',\n",
    "'무엇때문에','붕붕','윙윙','나','우리','엉엉','휘익','윙윙','오호','아하','어쨋든','만 못하다','하기보다는','차라리,하는 편이 낫다','흐흐','놀라다','상대적으로 말하자면','마치',\n",
    "'아니라면','쉿','그렇지 않으면','그렇지' '않다면','안 그러면','아니었다면','하든지','아니면','이라면','좋아','알았어','하는것도','그만이다','어쩔수 없다','하나','일','일반적으로',\n",
    "'일단','한켠으로는','오자마자','이렇게되면','이와같다면','전부','한마디','한항목','근거로','하기에','아울러','하지 않도록','않기 위해서','이르기까지','이 되다','로 인하여',\n",
    "'까닭으로','이유만으로','이로 인하여','그래서','이 때문에','그러므로','그런 까닭에','알 수 있다','결론을 낼 수 있다','으로 인하여','있다','어떤것','관계가 있다','관련이 있다',\n",
    "'연관되다','어떤것들','에 대해','이리하여','그리하여','여부','하기보다는','하느니','하면 할수록','운운','이러이러하다','하구나','하도다','다시말하면','다음으로','에 있다',\n",
    "'에 달려 있다','우리','우리들','오히려','하기는한데','어떻게','어떻해','어찌됏어','어때','어째서','본대로','자','이','이쪽','여기','이것','이번','이렇게말하자면','이런','이러한',\n",
    "'이와 같은','요만큼','요만한 것','얼마 안 되는 것','이만큼','이 정도의','이렇게 많은 것','이와 같다','이때','이렇구나','것과 같이','끼익','삐걱,따위','와 같은 사람들',\n",
    "'부류의 사람들','왜냐하면','중의하나','오직','오로지','에 한하다','하기만 하면','도착하다','까지 미치다','도달하다','정도에 이르다','할 지경이다','결과에 이르다','관해서는',\n",
    "'여러분','하고 있다','한 후','혼자','자기','자기집','자신','우에' '종합한것과같이','총적으로' '보면','총적으로' '말하면','총적으로','대로 하다','으로서','참','그만이다','할 따름이다','쿵',\n",
    "'탕탕','쾅쾅','둥둥','봐','봐라','아이야','아니','와아','응','아이','참나','년','월','일','령','영','일','이','삼','사','오','육','륙','칠','팔','구','이천육','이천칠','이천팔','이천구','하나','둘','셋',\n",
    "'넷','다섯','여섯','일곱','여덟','아홉','령','영','하다','의','가','은','들','는','종','잘','걍','과','도','를','으로','자','에','와','한','이다','다','ㅋㅋ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch(data):\n",
    "    temp=[]\n",
    "    for i in data:\n",
    "        if i not in stopwords:\n",
    "            if len(i) >= 2:\n",
    "                temp.append(re.sub('[ㄱ-ㅎ|ㅏ-ㅣ]+','',i))\n",
    "    return temp      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['khaiii']=train['khaiii'].apply(ch)\n",
    "train['mecab']=train['mecab'].apply(ch)\n",
    "train['okt']=train['okt'].apply(ch)\n",
    "\n",
    "test['khaiii']=test['khaiii'].apply(ch)\n",
    "test['mecab']=test['mecab'].apply(ch)\n",
    "test['okt']=test['okt'].apply(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_khaiii=[]\n",
    "word_index_khaiii={}\n",
    "wordCount_khaiii={}\n",
    "\n",
    "wordList_mecab=[]\n",
    "word_index_mecab={}\n",
    "wordCount_mecab={}\n",
    "\n",
    "wordList_okt=[]\n",
    "word_index_okt={}\n",
    "wordCount_okt={}\n",
    "\n",
    "for s in train['khaiii']: \n",
    "    for word in s:\n",
    "        if word not in wordList_khaiii:\n",
    "            wordCount_khaiii[word]=1\n",
    "            wordList_khaiii.append(word)\n",
    "        else:\n",
    "            wordCount_khaiii[word]=wordCount_khaiii[word]+1\n",
    "\n",
    "for s in train['mecab']: \n",
    "    for word in s:\n",
    "        if word not in wordList_mecab:\n",
    "            wordCount_mecab[word]=1\n",
    "            wordList_mecab.append(word)\n",
    "        else:\n",
    "            wordCount_mecab[word]=wordCount_mecab[word]+1  \n",
    "\n",
    "for s in train['okt']: \n",
    "    for word in s:\n",
    "        if word not in wordList_okt:\n",
    "            wordCount_okt[word]=1\n",
    "            wordList_okt.append(word)\n",
    "        else:\n",
    "            wordCount_okt[word]=wordCount_okt[word]+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCount_df=pd.DataFrame([[i,j] for i,j in wordCount.items()],columns=['word', 'index'])\n",
    "# wordCount_df.to_csv('wordCount_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCount_df=pd.read_csv('wordCount_df.csv',)\n",
    "# wordCount_df=wordCount_df.sort_values('index',ascending=False)\n",
    "# wordCount_df=wordCount_df[wordCount_df['index']>5]\n",
    "# wordCount={}\n",
    "# for i in range(len(wordCount_df)):\n",
    "#     wordCount[wordCount_df['word'].iloc[i]]=wordCount_df['index'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_khaiii=[]\n",
    "word_index_khaiii={}\n",
    "for index ,word in enumerate(wordCount_khaiii.keys()):\n",
    "    word_index_khaiii[word]=len(wordList_khaiii)\n",
    "    wordList_khaiii.append(word)\n",
    "    \n",
    "wordList_mecab=[]\n",
    "word_index_mecab={}\n",
    "for index ,word in enumerate(wordCount_mecab.keys()):\n",
    "    word_index_mecab[word]=len(wordList_mecab)\n",
    "    wordList_mecab.append(word)\n",
    "    \n",
    "wordList_okt=[]\n",
    "word_index_okt={}\n",
    "for index ,word in enumerate(wordCount_okt.keys()):\n",
    "    word_index_okt[word]=len(wordList_okt)\n",
    "    wordList_okt.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix , One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_khaiii_dtm_array=[]\n",
    "train_khaiii_ohe_array=[]\n",
    "for corpus in train['khaiii']:\n",
    "    temp1=[0]*len(wordList_khaiii)\n",
    "    temp2=[0]*len(wordList_khaiii)\n",
    "    for word in corpus:\n",
    "        if word in wordList_khaiii:\n",
    "            temp1[word_index_khaiii[word]]=+1\n",
    "            temp2[word_index_khaiii[word]]=1\n",
    "    train_khaiii_dtm_array.append(temp1)\n",
    "    train_khaiii_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mecab_dtm_array=[]\n",
    "train_mecab_ohe_array=[]\n",
    "for corpus in train['mecab']:\n",
    "    temp1=[0]*len(wordList_mecab)\n",
    "    temp2=[0]*len(wordList_mecab)\n",
    "    for word in corpus:\n",
    "        if word in wordList_mecab:\n",
    "            temp1[word_index_mecab[word]]=+1\n",
    "            temp2[word_index_mecab[word]]=1\n",
    "    train_mecab_dtm_array.append(temp1)\n",
    "    train_mecab_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_okt_dtm_array=[]\n",
    "train_okt_ohe_array=[]\n",
    "for corpus in train['okt']:\n",
    "    temp1=[0]*len(wordList_okt)\n",
    "    temp2=[0]*len(wordList_okt)\n",
    "    for word in corpus:\n",
    "        if word in wordList_okt:\n",
    "            temp1[word_index_okt[word]]=+1\n",
    "            temp2[word_index_okt[word]]=1\n",
    "    train_okt_dtm_array.append(temp1)\n",
    "    train_okt_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_khaiii_dtm=pd.DataFrame(train_khaiii_dtm_array,columns=word_index.keys())\n",
    "train_khaiii_ohe=pd.DataFrame(train_khaiii_ohe_array,columns=word_index.keys())\n",
    "train_mecab_dtm=pd.DataFrame(train_mecab_dtm_array,columns=word_index.keys())\n",
    "train_mecab_ohe=pd.DataFrame(train_mecab_ohe_array,columns=word_index.keys())\n",
    "train_okt_dtm=pd.DataFrame(train_okt_dtm_array,columns=word_index.keys())\n",
    "train_okt_ohe=pd.DataFrame(train_okt_ohe_array,columns=word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_khaiii_dtm.to_csv('train_khaiii_dtm.csv',index=False)\n",
    "train_khaiii_ohe.to_csv('train_khaiii_ohe.csv',index=False)\n",
    "train_mecab_dtm.to_csv('train_mecab_dtm.csv',index=False)\n",
    "train_mecab_ohe.to_csv('train_mecab_ohe.csv',index=False)\n",
    "train_okt_dtm.to_csv('train_okt_dtm.csv',index=False)\n",
    "train_okt_ohe.to_csv('train_okt_ohe.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_khaiii_dtm_array=[]\n",
    "test_khaiii_ohe_array=[]\n",
    "for corpus in test['khaiii']:\n",
    "    temp1=[0]*len(wordList_khaiii)\n",
    "    temp2=[0]*len(wordList_khaiii)\n",
    "    for word in corpus:\n",
    "        if word in wordList_khaiii:\n",
    "            temp1[word_index_khaiii[word]]=+1\n",
    "            temp2[word_index_khaiii[word]]=1\n",
    "    test_khaiii_dtm_array.append(temp1)\n",
    "    test_khaiii_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mecab_dtm_array=[]\n",
    "test_mecab_ohe_array=[]\n",
    "for corpus in test['mecab']:\n",
    "    temp1=[0]*len(wordList_mecab)\n",
    "    temp2=[0]*len(wordList_mecab)\n",
    "    for word in corpus:\n",
    "        if word in wordList_mecab:\n",
    "            temp1[word_index_mecab[word]]=+1\n",
    "            temp2[word_index_mecab[word]]=1\n",
    "    test_mecab_dtm_array.append(temp1)\n",
    "    test_mecab_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_okt_dtm_array=[]\n",
    "test_okt_ohe_array=[]\n",
    "for corpus in test['okt']:\n",
    "    temp1=[0]*len(wordList_okt)\n",
    "    temp2=[0]*len(wordList_okt)\n",
    "    for word in corpus:\n",
    "        if word in wordList_okt:\n",
    "            temp1[word_index_okt[word]]=+1\n",
    "            temp2[word_index_okt[word]]=1\n",
    "    test_okt_dtm_array.append(temp1)\n",
    "    test_okt_ohe_array.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_khaiii_dtm=pd.DataFrame(test_khaiii_dtm_array,columns=word_index.keys())\n",
    "test_khaiii_ohe=pd.DataFrame(test_khaiii_ohe_array,columns=word_index.keys())\n",
    "test_mecab_dtm=pd.DataFrame(test_mecab_dtm_array,columns=word_index.keys())\n",
    "test_mecab_ohe=pd.DataFrame(test_mecab_ohe_array,columns=word_index.keys())\n",
    "test_okt_dtm=pd.DataFrame(test_okt_dtm_array,columns=word_index.keys())\n",
    "test_okt_ohe=pd.DataFrame(test_okt_ohe_array,columns=word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_khaiii_dtm.to_csv('test_khaiii_dtm.csv',index=False)\n",
    "test_khaiii_ohe.to_csv('test_khaiii_ohe.csv',index=False)\n",
    "test_mecab_dtm.to_csv('test_mecab_dtm.csv',index=False)\n",
    "test_mecab_ohe.to_csv('test_mecab_ohe.csv',index=False)\n",
    "test_okt_dtm.to_csv('test_okt_dtm.csv',index=False)\n",
    "test_okt_ohe.to_csv('test_okt_ohe.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xTrain yTrain xTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_khaiii_dtm=pd.read_csv('train_khaiii_dtm.csv')\n",
    "train_khaiii_ohe=pd.read_csv('train_khaiii_ohe.csv')\n",
    "train_mecab_dtm=pd.read_csv('train_mecab_dtm.csv')\n",
    "train_mecab_ohe=pd.read_csv('train_mecab_ohe.csv')\n",
    "train_okt_dtm=pd.read_csv('train_okt_dtm.csv')\n",
    "train_okt_ohe=pd.read_csv('train_okt_ohe.csv')\n",
    "\n",
    "test_khaiii_dtm=pd.read_csv('test_khaiii_dtm.csv')\n",
    "test_khaiii_ohe=pd.read_csv('test_khaiii_ohe.csv')\n",
    "test_mecab_dtm=pd.read_csv('test_mecab_dtm.csv')\n",
    "test_mecab_ohe=pd.read_csv('test_mecab_ohe.csv')\n",
    "test_okt_dtm=pd.read_csv('test_okt_dtm.csv')\n",
    "test_okt_ohe=pd.read_csv('test_okt_ohe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=train_ohe.values\n",
    "yTrain=train['label'].values\n",
    "xTest=test_ohe.values\n",
    "yTest=test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CategoricalNB()\n",
    "model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NB=model.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(xTrain , yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_NB']=pred_NB\n",
    "(test['label']==test['pred_NB']).sum()/test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dtm):\n",
    "    n=dtm.shape[0]\n",
    "    c=(dtm!=0).sum(axis=0)\n",
    "    m=np.log(n/(c+1))\n",
    "    return m*dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ifidf=tfidf(xTrain)\n",
    "test_ifidf=tfidf(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = train_dtm[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression()\n",
    "    x_nb = train_dtm.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,r = get_mdl(train['label'])\n",
    "preds=m.predict_proba(test_dtm.multiply(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NB_SVM=[]\n",
    "for i in preds:\n",
    "    pred_NB_SVM.append(i.argmax())\n",
    "\n",
    "test['pred_NB_SVM']=pred_NB_SVM\n",
    "(test['label']==test['pred_NB_SVM']).sum()/test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(2048,input_shape=(xTrain.shape[1],) ,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(2,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['none']=train['label']==0\n",
    "train['hate']=train['label']==1\n",
    "yTrain=train[['none','hate']].values\n",
    "yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = xTrain[:2000]\n",
    "partial_x_train = xTrain[2000:]\n",
    "\n",
    "y_val = yTrain[:2000]\n",
    "partial_y_train = yTrain[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo',label=\"Training loss\" )\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf() #그래프 초기화 \n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('RNN_model_v010.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_RNN=model.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_RNN_list=[]\n",
    "for i in range(len(pred_RNN)):\n",
    "    pred_RNN_list.append(pred_RNN[i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_RNN']=pred_RNN_list\n",
    "(test['label']==test['pred_RNN']).sum()/test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
